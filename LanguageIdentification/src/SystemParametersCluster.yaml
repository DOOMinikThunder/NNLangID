# System parameters
create_splitted_data_files: True                             # split into training, validation and test set from an original file
train_embed: True
train_rnn: True
eval_test_set: True
run_terminal: False                                          # if True: disables all other calculations

# Print parameters
print_embed_testing: True
print_model_checkpoint_embed_weights: null #"../data/save/trained/embed_weights.txt" #None
print_model_checkpoint: null #"../data/save/trained/rnn_model_checkpoint.pth" #None

# Data paths
input_tr_va_te_data_rel_path: "../data/input_data/original/recall_oriented_dl.csv"   # training, validation and test will be generated from this file
input_rt_data_rel_path: "../data/input_data/original/uniformly_sampled_dl.csv"  # to change later, rt = real test

out_tr_data_rel_path: "../data/input_data/original_splitted/training.csv"
out_va_data_rel_path: "../data/input_data/original_splitted/validation.csv"
out_te_data_rel_path: "../data/input_data/original_splitted/test.csv"

# Save and load paths for embedding weights and model checkpoints
embed_weights_rel_path: "../data/save/embed_weights.txt"
embed_model_checkpoint_rel_path: "../data/save/embed_model_checkpoint.pth"
rnn_model_checkpoint_rel_path: "../data/save/rnn_model_checkpoint.pth"

trained_embed_weights_rel_path: "../data/save/trained/embed_weights.txt"
trained_model_checkpoint_rel_path: "../data/save/trained/rnn_model_checkpoint.pth"

# Data manipulation parameters
tr_va_te_split_ratios: [0.8, 0.1, 0.1]                      # [train_ratio, val_ratio, test_ratio]
split_shuffle_seed: 42                                      # ensures that splitted sets (training, validation, test) are always created identically (given a specified ratio)
fetch_only_langs: null #['de', 'en', 'es', 'fr', 'it'] #['de', 'en', 'es'] #['pl', 'sv'] #['el', 'fa', 'hi', 'ca'] #None
fetch_only_first_x_tweets: .inf
min_char_frequency: 2                                       # chars appearing less than min_char_frequency in the training set will not be used to create the vocabulary vocab_chars

# HYPERPARAMETERS EMBEDDING
sampling_table_min_char_count: 1                            # determines the precision of the sampling (should be 1 or higher)
sampling_table_specified_size_cap: .inf #10000 #.inf              # caps specified sampling table size to this value (no matter how big it would be according to sampling_table_min_char_count)
                                                            # note: this is only the specified size, the actual table size may slightly deviate due to roundings in the calculation
max_context_window_size: 3
num_neg_samples: 5
batch_size_embed: 1000
max_eval_checks_not_improved_embed: 10
max_num_epochs_embed: 1 #.inf
eval_every_num_batches_embed: 10000
lr_decay_every_num_batches_embed: 10000
lr_decay_factor_embed: 1.0                                  # 1.0 means no decay
initial_lr_embed: 0.025

# HYPERPARAMETERS RNN
hidden_size_rnn: 100
num_layers_rnn: 1
is_bidirectional: True
batch_size_rnn: 100
max_eval_checks_not_improved_rnn: 10
max_num_epochs_rnn: 10 #.inf
eval_every_num_batches_rnn: 1000
lr_decay_every_num_batches_rnn: 1000
lr_decay_factor_rnn: 1.0                                    # 1.0 means no decay
initial_lr_rnn: 0.01
weight_decay_rnn: 0.00001