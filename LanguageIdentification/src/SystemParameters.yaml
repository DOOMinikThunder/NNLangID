# System parameters
create_splitted_data_files: True                             # split into training, validation and test set from an original file
calc_embed: True
train_rnn: True
eval_test_set: True
use_terminal: False                                          # if True: disables all other calculations
terminal_live_tweets: True                                   # change this if you want to sample live tweets

# Print parameters
print_embed_testing: True
print_model_checkpoint_embed_weights: None #"../data/save/trained/embed_weights_de_en_es_fr_it.txt"#None
print_model_checkpoint: None #"../data/save/trained/rnn_model_checkpoint_de_en_es_fr_it.pth"#None

# Data paths
input_tr_va_te_data_rel_path: "../data/input_data/testing/test_recall_de_en_es.csv"   # training, validation and test will be generated from this file
input_rt_data_rel_path: "../data/input_data/original/uniformly_sampled_dl.csv"  # to change later, rt = real test

out_tr_data_rel_path: "../data/input_data/original_splitted/training.csv"
out_va_data_rel_path: "../data/input_data/original_splitted/validation.csv"
out_te_data_rel_path: "../data/input_data/original_splitted/test.csv"

# Save and load paths for embedding weights and model checkpoints
embed_weights_rel_path: "../data/save/embed_weights.txt"
embed_model_checkpoint_rel_path: "../data/save/embed_model_checkpoint.pth"
rnn_model_checkpoint_rel_path: "../data/save/rnn_model_checkpoint.pth"

trained_embed_weights_rel_path: "../data/save/trained/embed_weights_de_en_es_fr_it.txt"
trained_model_checkpoint_rel_path: "../data/save/trained/rnn_model_checkpoint_de_en_es_fr_it.pth"

# Data manipulation parameters
tr_va_te_split_ratios: [0.8, 0.1, 0.1]                      # [train_ratio, val_ratio, test_ratio]
split_shuffle_seed: 42                                      # ensures that splitted sets (training, validation, test) are always created identically (given a specified ratio)
fetch_only_langs: None #['de', 'en', 'es', 'fr', 'it'] #['de', 'en', 'es']#['el', 'fa', 'hi', 'ca']#None
fetch_only_first_x_tweets: 100000000 #math.inf
min_char_frequency: 2                                       # chars appearing less than min_char_frequency in the training set will not be used to create the vocabulary vocab_chars

# HYPERPARAMETERS EMBEDDING
sampling_table_min_char_count: 1                            # determines the precision of the sampling (should be 1 or higher)
sampling_table_specified_size_cap: 10000#100000000 #math.inf      # caps specified sampling table size to this value (no matter how big it would be according to sampling_table_min_char_count)
                                                            # note: this is only the specified size, the actual table size may slightly deviate due to roundings in the calculation
#    embed_dim: 2                                           # will be set automatically later to: roundup(log2(vocabulary-size))
max_context_window_size: 3
num_neg_samples: 5
batch_size_embed: 1
max_eval_checks_not_improved_embed: 1
max_num_epochs_embed: 1 #100000000 #math.inf
eval_every_num_batches_embed: 1000
lr_decay_every_num_batches_embed: 1000
lr_decay_factor_embed: 1.0                                  # 1.0 means no decay
initial_lr_embed: 0.025

# HYPERPARAMETERS RNN
hidden_size_rnn: 100
num_layers_rnn: 1
is_bidirectional: True
batch_size_rnn: 10
initial_lr_rnn: 0.01
scheduler_step_size_rnn: 1                                  # currently not functioning
scheduler_gamma_rnn: 0.1                                    # currently not functioning
weight_decay_rnn: 0.00001
num_epochs_rnn: 10 #100000000 #math.inf